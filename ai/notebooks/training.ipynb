{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be0c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "from plant_care_ai.data.dataset import PlantNetDataset\n",
    "from plant_care_ai.data.preprocessing import get_training_pipeline, get_inference_pipeline\n",
    "from plant_care_ai.models.resnet18 import Resnet18\n",
    "from plant_care_ai.models.effecientnetv2 import create_efficientnetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd837ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0107cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"model\": \"efficientnetv2\",\n",
    "    \"variant\": \"b0\",\n",
    "    \n",
    "    \"subset_classes\": 50, #how many classes\n",
    "    \"subset_samples_per_class\": 50, # max samples/ class\n",
    "    \"train_samples_per_class\": 50,\n",
    "    \"val_samples_per_class\": 15,\n",
    "    \n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 15,\n",
    "    \"lr\": 0.001,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \n",
    "    \"data_dir\": \"../data/plantnet_300K\",\n",
    "    \"img_size\": 224,\n",
    "    \"augm_strength\": 0.7,\n",
    "    \n",
    "    \"checkpoint_dir\": \"../checkpoints/notebook_exp\",\n",
    "    \"experiment_name\": f\"exp_{int(time.time())}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d99f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 243916 samples, 1081 classes\n",
      "Loaded 31118 samples, 1081 classes\n",
      "['1355868', '1355920', '1355932', '1355936', '1355937', '1355955', '1355959', '1355961', '1355978', '1355990', '1356003', '1356022', '1356037', '1356055', '1356075', '1356076', '1356111', '1356126', '1356138', '1356257', '1356278', '1356279', '1356309', '1356379', '1356380', '1356382', '1356420', '1356421', '1356428', '1356469', '1356692', '1356781', '1356816', '1356847', '1356901', '1357330', '1357331', '1357367', '1357379', '1357506', '1357635', '1357652', '1357677', '1357681', '1357682', '1357705', '1358094', '1358095', '1358096', '1358097']\n"
     ]
    }
   ],
   "source": [
    "train_tfm = get_training_pipeline(CONFIG[\"img_size\"], CONFIG[\"augm_strength\"])\n",
    "val_tfm = get_inference_pipeline(CONFIG[\"img_size\"])\n",
    "\n",
    "#loading full datasets\n",
    "train_dataset = PlantNetDataset(CONFIG[\"data_dir\"], \"train\", train_tfm)\n",
    "val_dataset = PlantNetDataset(CONFIG[\"data_dir\"], \"val\", val_tfm)\n",
    "\n",
    "#loading its subsets\n",
    "all_classes = sorted(train_dataset.classes)\n",
    "selected_classes = all_classes[:CONFIG[\"subset_classes\"]]\n",
    "\n",
    "print(selected_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f965ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1915\n",
      "Val samples: 520\n",
      "Train batches: 60\n",
      "Val batches: 17\n"
     ]
    }
   ],
   "source": [
    "train_indices = []\n",
    "train_class_counts = {}\n",
    "\n",
    "for idx, (_, species_id) in enumerate(train_dataset.paths):\n",
    "    if species_id not in selected_classes:\n",
    "        continue\n",
    "    count = train_class_counts.get(species_id, 0)\n",
    "    if count < CONFIG[\"train_samples_per_class\"]:\n",
    "        train_indices.append(idx)\n",
    "        train_class_counts[species_id] = count + 1\n",
    "\n",
    "val_indices = []\n",
    "val_class_counts = {}\n",
    "\n",
    "for idx, (_, species_id) in enumerate(val_dataset.paths):\n",
    "    if species_id not in selected_classes:\n",
    "        continue\n",
    "    count = val_class_counts.get(species_id, 0)\n",
    "    if count < CONFIG[\"val_samples_per_class\"]:\n",
    "        val_indices.append(idx)\n",
    "        val_class_counts[species_id] = count + 1\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "print(f\"Train samples: {len(train_subset)}\")\n",
    "print(f\"Val samples: {len(val_subset)}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8613483e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "\n",
      "Model: EfficientNetV2-B0\n",
      "\tParameters: 69,770,010\n",
      "\tTrainable parameters: 69,770,010\n",
      "\tModel size: 266.15 MB\n",
      "\tDevice: cpu\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(selected_classes)\n",
    "print(num_classes)\n",
    "\n",
    "if CONFIG[\"model\"] == \"resnet18\":\n",
    "    model = Resnet18(num_classes=num_classes)\n",
    "    model_name = \"ResNet18\"\n",
    "elif CONFIG[\"model\"] == \"efficientnetv2\":\n",
    "    model = create_efficientnetv2(\n",
    "        variant=CONFIG[\"variant\"],\n",
    "        num_classes=num_classes,\n",
    "    )\n",
    "    model_name = f\"EfficientNetV2-{CONFIG['variant'].upper()}\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model: {CONFIG['model']}\")\n",
    "\n",
    "model = model.to(CONFIG[\"device\"])\n",
    "\n",
    "# get the stats\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel: {model_name}\")\n",
    "print(f\"\\tParameters: {total_params:,}\")\n",
    "print(f\"\\tTrainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\tModel size: {total_params * 4 / (1024**2):.2f} MB\")\n",
    "print(f\"\\tDevice: {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9654cd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimizer: AdamW\n",
      "\tLearning rate: 0.001\n",
      "\tWeight decay: 0.01\n",
      "\n",
      "Scheduler: CosineAnnealingLR\n",
      "\tT_max: 15\n",
      "\n",
      "Loss: CrossEntropyLoss\n",
      "\tLabel smoothing: 0.1\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG[\"label_smoothing\"])\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=CONFIG[\"epochs\"],\n",
    "    eta_min=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimizer: AdamW\")\n",
    "print(f\"\\tLearning rate: {CONFIG['lr']}\")\n",
    "print(f\"\\tWeight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"\\nScheduler: CosineAnnealingLR\")\n",
    "print(f\"\\tT_max: {CONFIG['epochs']}\")\n",
    "print(f\"\\nLoss: CrossEntropyLoss\")\n",
    "print(f\"\\tLabel smoothing: {CONFIG['label_smoothing']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1adbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint dir: ../checkpoints/notebook_exp/exp_1767623821\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = Path(CONFIG[\"checkpoint_dir\"]) / CONFIG[\"experiment_name\"]\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"\\nCheckpoint dir: {checkpoint_dir}\")\n",
    "\n",
    "with open(checkpoint_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": [],\n",
    "    \"val_top5\": [],\n",
    "    \"lr\": [],\n",
    "}\n",
    "\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd72f725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Train\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, pred = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += pred.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"acc\": f\"{100.*correct/total:.1f}%\"\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(loader), 100.0 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41d35d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Val\", leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # top 1\n",
    "            _, pred = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct_top1 += pred.eq(labels).sum().item()\n",
    "            \n",
    "            # top 5 res\n",
    "            _, top5 = outputs.topk(min(5, outputs.size(1)), 1, True, True)\n",
    "            correct_top5 += top5.eq(labels.view(-1, 1).expand_as(top5)).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"acc\": f\"{100.*correct_top1/total:.1f}%\"\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    top1_acc = 100.0 * correct_top1 / total\n",
    "    top5_acc = 100.0 * correct_top5 / total\n",
    "    \n",
    "    return avg_loss, top1_acc, top5_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "436d9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: EfficientNetV2-B0\n",
      "Epochs: 15\n",
      "Dataset: 1915 train, 520 val\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e60c6fba8e549689e93f3f675155f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szczuru/.local/lib/python3.14/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, CONFIG[\u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_loss, train_acc = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     val_loss, val_acc, val_top5 = validate(\n\u001b[32m     15\u001b[39m         model, val_loader, criterion, CONFIG[\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     16\u001b[39m     )\n\u001b[32m     18\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      5\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m pbar = tqdm(loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTrain\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/utils/data/dataloader.py:494\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/utils/data/dataloader.py:427\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/torch/utils/data/dataloader.py:1170\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1163\u001b[39m w.daemon = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[32m   1166\u001b[39m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[32m   1169\u001b[39m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[43mw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[38;5;28mself\u001b[39m._index_queues.append(index_queue)\n\u001b[32m   1172\u001b[39m \u001b[38;5;28mself\u001b[39m._workers.append(w)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/context.py:224\u001b[39m, in \u001b[36mProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mProcess\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/context.py:300\u001b[39m, in \u001b[36mForkServerProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_forkserver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/popen_forkserver.py:35\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/popen_fork.py:20\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.14/multiprocessing/popen_forkserver.py:58\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = util.Finalize(\u001b[38;5;28mself\u001b[39m, util.close_fds,\n\u001b[32m     56\u001b[39m                                (_parent_w, \u001b[38;5;28mself\u001b[39m.sentinel))\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mself\u001b[39m.pid = forkserver.read_signed(\u001b[38;5;28mself\u001b[39m.sentinel)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Dataset: {len(train_subset)} train, {len(val_subset)} val\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']}\")\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, CONFIG[\"device\"]\n",
    "    )\n",
    "    \n",
    "    val_loss, val_acc, val_top5 = validate(\n",
    "        model, val_loader, criterion, CONFIG[\"device\"]\n",
    "    )\n",
    "    \n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_top5\"].append(val_top5)\n",
    "    history[\"lr\"].append(current_lr)\n",
    "    \n",
    "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
    "    print(f\"Val:   Loss={val_loss:.4f}, Top-1={val_acc:.2f}%, Top-5={val_top5:.2f}%\")\n",
    "    print(f\"LR: {current_lr:.6f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_acc\": val_acc,\n",
    "            \"val_top5\": val_top5,\n",
    "            \"config\": CONFIG,\n",
    "        }, checkpoint_dir / \"best.pth\")\n",
    "        print(f\"Model saved: (acc: {val_acc:.2f}%)\")\n",
    "\n",
    "torch.save({\n",
    "    \"epoch\": CONFIG[\"epochs\"],\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"history\": history,\n",
    "    \"config\": CONFIG,\n",
    "}, checkpoint_dir / \"last.pth\")\n",
    "\n",
    "with open(checkpoint_dir / \"history.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "629e55ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions:\n",
      "#   True     Pred     Conf     Status\n",
      "----------------------------------------\n",
      "1   5        0           2.0% Wrong\n",
      "2   5        0           2.0% Wrong\n",
      "3   9        0           2.0% Wrong\n",
      "4   9        0           2.0% Wrong\n",
      "5   9        0           2.0% Wrong\n",
      "6   9        0           2.0% Wrong\n",
      "7   9        0           2.0% Wrong\n",
      "8   9        0           2.0% Wrong\n",
      "\n",
      "Batch accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# interference phase\n",
    "model.eval()\n",
    "\n",
    "# get a batch from validation\n",
    "test_images, test_labels = next(iter(val_loader))\n",
    "test_images = test_images[:8].to(CONFIG[\"device\"])\n",
    "test_labels = test_labels[:8]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_images)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    confidences, predictions = torch.max(probs, dim=1)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(f\"{'#':<3} {'True':<8} {'Pred':<8} {'Conf':<8} {'Status'}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(test_labels)):\n",
    "    true_label = test_labels[i].item()\n",
    "    pred_label = predictions[i].item()\n",
    "    confidence = confidences[i].item() * 100\n",
    "    \n",
    "    status = \"Correct\" if true_label == pred_label else \"Wrong\"\n",
    "    if true_label == pred_label:\n",
    "        correct += 1\n",
    "    \n",
    "    print(f\"{i+1:<3} {true_label:<8} {pred_label:<8} {confidence:>6.1f}% {status}\")\n",
    "\n",
    "print(f\"\\nBatch accuracy: {100*correct/len(test_labels):.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
